#!/usr/bin/env snakemake


import os
import sys


shell.executable("bash")


workdir: "."
configfile: "config.yaml"


rule all:
    input:
        expand("SC2.{barcode}.fin", barcode=config["barcodes"].keys()),
	ancient('IRMA/IRMA_reads2hadoop.fin'),
	ancient('hadoop/amplicon2hadoop.fin'),
	ancient('hadoop/irmaConsensus2hadoop.fin')
    shell:
        'python {workflow.basedir}/scripts/addColumns.py --sc2 --benchmark -o hadoop/benchmarks.txt -f log/benchmarks/* && python {workflow.basedir}/scripts/hadoopReloader.py -c tests/config.yaml -d hadoop'

rule gather_fastqs:
    input:
        "config.yaml"
    output:
        "IRMA/cat_{barcode}.fastq"
    params:
        experiment_id = lambda wildcards: config["barcodes"][wildcards.barcode]["experiment_id"],
        sample_id = lambda wildcards: config["barcodes"][wildcards.barcode]["sample_id"],
        flow_cell_id = lambda wildcards: config["barcodes"][wildcards.barcode]["flow_cell_id"]
    message: "Step 1 - merging all read files into a single file"
    shell:
        "cat /scicomp/groups/Projects/SARS2Seq/data/by-instrument/18-6-605_Nanopore-GridION-GXB02142/{params.experiment_id}/{params.sample_id}/*{params.flow_cell_id}*/fastq_pass/{wildcards.barcode}/{params.flow_cell_id}_pass_{wildcards.barcode}*.fastq > {output}"

rule plot_read_lengths:
    input:
        "IRMA/cat_{barcode}.fastq"
    output:
        "qa/{barcode}.png"
    log:
        out = "log/{barcode}.plot-read-lengths.stdout.log",
        err = "log/{barcode}.plot-read-lengths.stderr.log"
    group:
        "trim-map"
    conda:
        "envs/plot.read-lengths.yaml"
    threads: 16
    message: "Step X - plotting sequence read lengths"
    script:
        "{workflow.basedir}/scripts/plot.read-lengths.py"
        " --indir IRMA"
        " --prefix {barcode}"
        " --cpus {threads}"
        " --outfile {output}"
        " --title 'Sample_ID and Experiment_ID'"
        " --subtitle {barcode}"
        " 1> {log.out}"
        " 2> {log.err}"


rule barcode_trim_left:
    input:
        "IRMA/cat_{barcode}.fastq"
    output:
        "IRMA/{barcode}_bartrim_l.fastq"
    log:
        out = "log/{barcode}.bbduk.trim_left.stdout.log",
        err = "log/{barcode}.bbduk.trim_left.stderr.log"
    params:
        barcode_sequence = lambda wildcards: config["barcodes"][wildcards.barcode]["barcode_sequence"]
    group:
        "trim-map"
    # conda:
    #     "envs/bbtools.yaml"
    threads: 16
    message: "Step 2 - trimming left barcode"
    shell:
        "/apps/x86_64/bbmap/38.84/bbduk.sh"
        " in={input}"
        " out={output}"
        " hdist=3"
        " literal={params.barcode_sequence}"
        " ktrim=l"
        " k=17"
        " qin=33"
        " rcomp=f"
        " threads={threads}"
        " 1> {log.out}"
        " 2> {log.err}"


rule summarize_barcode_trim_left:
    input:
        "log/{barcode}.bbduk.trim_left.stderr.log"
    output:
        "qa/stats.{barcode}_left_barcode_trimming.tsv"
    group:
        "trim-map"
    message: "Step X - summarizing left barcode trimming"
    shell:
        "scripts/grep_summarize_bbduk_log.sh {input} > {output}"


rule barcode_trim_right:
    input:
        "IRMA/{barcode}_bartrim_l.fastq"
    output:
        "IRMA/{barcode}_bartrim_lr.fastq"
    log:
        out = "log/{barcode}.bbduk.trim_right.stdout.log",
        err = "log/{barcode}.bbduk.trim_right.stderr.log"
    params:
        barcode_sequence = lambda wildcards: config["barcodes"][wildcards.barcode]["barcode_sequence"]
    group:
        "trim-map"
    # conda:
    #     "envs/bbtools.yaml"
    threads: 16
    message: "Step 3 - trimming right barcode"
    shell:
        "/apps/x86_64/bbmap/38.84/bbduk.sh"
        " in={input}"
        " out={output}"
        " hdist=3"
        " literal={params.barcode_sequence}"
        " ktrim=r"
        " k=17"
        " qin=33"
        " rcomp=f"
        " threads={threads}"
        " 1> {log.out}"
        " 2> {log.err}"


rule summarize_barcode_trim_right:
    input:
        "log/{barcode}.bbduk.trim_right.stderr.log"
    output:
        "qa/stats.{barcode}_right_barcode_trimming.tsv"
    group:
        "trim-map"
    message: "Step X - summarizing right barcode trimming"
    script:
        "scripts/grep_summarize_bbduk_log.sh {input} > {output}"


rule cutadapt:
    input:
        "IRMA/{barcode}_bartrim_lr.fastq"
    output:
        "IRMA/{barcode}_bartrim_lr_cutadapt.fastq"
    log:
        out = "log/{barcode}.cutadapt.stdout.log",
        err = "log/{barcode}.cutadapt.stderr.log"
    group:
        "trim-map"
    conda:
        "envs/cutadapt.yaml"
    message: "Step 4 - clipping reads on both sides"
    shell:
        "cutadapt -u 30 -u -30 -o {output} {input} 1> {log.out} 2> {log.err}"


rule summarize_cutadapt:
    input:
        "log/{barcode}.cutadapt.stdout.log"
    output:
        "qa/stats.{barcode}.cutadapt.tsv"
    group:
        "trim-map"
    message: "Step X - summarizing cutadapt trimming"
    script:
        """
        cnt_input_reads=$(grep '^Total reads' {input} | awk '{print $4}')
        cnt_input_bases=$(grep '^Total basepairs' {input} | awk '{print $4}' | sed 's/,//g')
        cnt_adapters=$(grep '^Reads with adapters:' {input} | awk '{print $4}' | sed 's/,//g')
        cnt_output_reads=$(( "${cnt_input_reads}" - "${cnt_adapters}" ))
        cnt_output_bases=$(grep '^Total written' {input} | awk '{print $4}' | sed 's/,//g')
        nfo="${cnt_input_reads} input reads\t${cnt_input_bases} input bases\t"
        nfo+="${cnt_output_reads} output reads \t${cnt_output_bases} output bases"
        echo -e "${nfo}" > {output}
        """


# rule subsample:
#     input:
#         "IRMA/{barcode}_bartrim_lr_cutadapt.fastq"
#     output:
#         "IRMA/{barcode}_bartrim_lr_cutadapt.fastq"
#     log:
#         # out = "log/{barcode}.seqtk.stdout.log",
#         err = "log/{barcode}.seqtk.stderr.log"
#     group:
#         "trim-map"
#     conda:
#         "envs/seqtk.yaml"
#     message: "Step 4b - subsampling cleaned up read if excess > 1M exist"
#     shell:
#         "seqtk sample -s608 {input} 1000000 1> {output} 2> {log.err}"


rule subsample:
    input:
        "IRMA/{barcode}_bartrim_lr_cutadapt.fastq"
    output:
        "IRMA/{barcode}_bartrim_lr_cutadapt_subsampled.fastq"
    log:
        out = "log/{barcode}.reformat.stdout.log",
        err = "log/{barcode}.reformat.stderr.log"
    group:
        "trim-map"
    # conda:
    #     "envs/bbtools.yaml"
    message: "Step 4b - subsampling cleaned up read if excess > 1M exist"
    shell:
        "/apps/x86_64/bbmap/38.84/reformat.sh"
        " in={input}"
        " out={output}"
        " samplereadstarget=1000000"
        " qin=33"
        " 1> {log.out}"
        " 2> {log.err}"


rule summarize_subsample:
    input:
        "log/{barcode}.reformat.stderr.log"
    output:
        "qa/stats.{barcode}.subsample.tsv"
    group:
        "trim-map"
    message: "Step X - summarizing subsampling"
    script:
        """
        cnt_input_reads=$(grep '^Input:' {input} | awk '{print $2}')
        cnt_input_bases=$(grep '^Input:' {input} | awk '{print $5}')
        cnt_output_reads=$(grep '^Output:' {input} | awk '{print $2}')
        cnt_output_bases=$(grep '^Output:' {input} | awk '{print $5}')
        cnt_discarded_reads=$(( "${cnt_input_reads}" - "${cnt_output_reads}" ))
        cnt_discarded_bases=$(( "${cnt_input_bases}" - "${cnt_output_bases}" ))
        nfo="${cnt_discarded_reads} discarded reads\t"
        nfo+="${cnt_discarded_bases} discarded bases"
        echo -e "${nfo}" > {output}
        """


rule irma:
    input:
        "IRMA/{barcode}_bartrim_lr_cutadapt_subsampled.fastq"
    output:
        touch("IRMA/{barcode}.irma.fin")
    log:
        out = "log/{barcode}.irma.stdout.log",
        err = "log/{barcode}.irma.stderr.log"
    group:
        "trim-map"
    threads: 16
    message: "Step 5 - assembling genome"
    shell:
        '{workflow.basedir}/scripts/irmawrapper.sh {config[irma_module]} {input} {wildcards.barcode} 1> {log.out} 2> {log.err}'


rule quast:
    input:
        "IRMA/{barcode}/SARS-CoV-2.fasta"
    output:
        temp("qa/.tmp/{barcode}/transposed_report.tsv")
    log:
        out = "log/{barcode}.quast.stdout.log",
        err = "log/{barcode}.quast.stderr.log"
    group:
        "trim-map"
    conda:
        "envs/quast.yaml"
    threads: 16
    message: "Step 5b - calculating assembly metrics"
    shell:
        "quast"
        " --fast"
        " --threads {threads}"
        " --output-dir qa/.tmp/{barcode}"
        " {input}"


rule report_assembly_statistics:
    input:
        "qa/.tmp/{barcode}/transposed_report.tsv"
    output:
        "qa/{barcode}-assembly-stats.tsv"
    group:
        "trim-map"
    message: "Step 5c - moving assembly metrics"
    shell:
        "mv {input} {output}"

# Pipeline waits here for all samples to produce the checkpoint input needed
#  here and then reevaluates the needed DAG for each sample.
checkpoint checkirma:
    input:
        ancient('IRMA/{barcode}.irma.fin')
    output:
        'IRMA/{barcode}.irma.decision'
    log:
        "log/irma/checkirma_{barcode}.log"
    shell:
        "[[ -s IRMA/{wildcards.barcode}/SARS-CoV-2.bam ]] &&"
        " echo passed > {output} ||"
        " echo failed > {output}"


rule irmareads2hadoop:
    input:
        ancient(expand('IRMA/{barcode}.irma.decision', barcode=config['barcodes'].keys()))
    output:
        temp('IRMA/IRMA_reads2hadoop.fin')
    log:
        "log/irma/irmareads2hadoop_all.log"
    benchmark:
        "log/benchmarks/irmareads2hadoop_all.log"
    shell:
        'python {workflow.basedir}/scripts/irmaBasicReadStats2hadoop.py -s {input} && touch {output}'

def passed_irma(wildcards):
    with checkpoints.checkirma.get(barcode=wildcards.barcode).\
    output[0].open() as f:
        if f.read().strip() == "passed":
            return 'hadoop/dais.fin'
        else:
            return "IRMA_negative/{barcode}"


rule pass_negatives:
    input:
        "IRMA/{barcode}.irma.decision"
    output:
        "IRMA_negative/{barcode}"
    shell:
        "touch {output}"

rule bamsort:
    input:
        'IRMA/{barcode}.irma.decision'
    output:
        'IRMA/{barcode}_SARS-CoV-2_sorted.bam'
    threads: 14
    group:
        'amplicon-cov'
    log:
        "log/bamsort/bamsort_{barcode}.log"
    shell:
        'hostname && /scicomp/home-pure/sars2seq/bin/samtools-1.11/samtools sort IRMA/{wildcards.barcode}/SARS-CoV-2.bam -o {output} --threads {threads} 2> {log} || touch {output}'

#Set library primers and PCR Library files according to Library in config
primers={'swift':{'bedpe':'/scicomp/home-pure/sars2seq/Resources/primers/SNAP_v2_amplicon_panel_IRMAref.bedpe','fasta':'/scicomp/home-pure/sars2seq/Resources/primers/SNAP_v2_amplicon_panel.fasta'},'4pool':{'bedpe':'/scicomp/home-pure/sars2seq/Resources/primers/SC2_200710-bypool200814_IRMAref.bedpe','fasta':'/scicomp/home-pure/sars2seq/Resources/primers/SC2_200710-bypool200814.fasta'},'articv3':{'bedpe':'/scicomp/home-pure/sars2seq/Resources/primers/artic_v3.bedpe','fasta':'/scicomp/home-pure/sars2seq/Resources/primers/artic_v3.fasta'},'sgene_v1':{'bedpe':'/scicomp/home-pure/sars2seq/Resources/primers/sgene_v1.bedpe','fasta':'/scicomp/home-pure/sars2seq/Resources/primers/sgene_v1.fasta'}}

rule amplicov:
    input:
        'IRMA/{barcode}_SARS-CoV-2_sorted.bam'
    output:
        'IRMA/{barcode}_amplicon_coverage.txt'
    group:
        'amplicon-cov'
    log:
        "log/amplicov/{barcode}.log"
    benchmark:
        "log/benchmarks/amplicov_{barcode}.log"
    params:
        bedpe = lambda wildcards: primers[config["barcodes"][wildcards.barcode]["Library"]]["bedpe"]
    shell:
        'hostname && [[ -f {input} ]] && python {workflow.basedir}/scripts/amplicov.py --bedpe {params.bedpe} --bam {input} -o IRMA -p {wildcards.barcode} 2> {log} && touch {output} || touch {output}'


rule cat_amplicov:
    input:
        expand('IRMA/{barcode}_amplicon_coverage.txt', barcode=config['barcodes'].keys())
    output:
        'hadoop/all_amplicon_coverage.txt'
    group:
        'amplicov2hadoop'
    log:
        "log/amplicov/cat.log"
    benchmark:
        "log/benchmarks/catamplicov_all.log"
    shell:
        'hostname && python {workflow.basedir}/scripts/addColumns.py -f {input} -o {output} --sc2 2> {log} && touch {output} || touch {output}'

rule amplicov2hadoop:
    input:
        'hadoop/all_amplicon_coverage.txt'
    output:
        touch('hadoop/amplicon2hadoop.fin')
    group:
        'amplicon2hadoop'
    log:
        'log/amplicov/hadoopload.log'
    benchmark:
        "log/benchmarks/amplicov2hadoop_all.log"
    shell:
        'hostname && {workflow.basedir}/scripts/hput {input} /user/nbx0/sars_cov2/amplicon_coverage/{config[runid]}_amplicon_coverage.txt && {workflow.basedir}/scripts/himpala "refresh amplicon_coverage" sars_cov2  2> {log}' #add hput and himpala later

rule cat_allAlleles:
    input:
        expand('IRMA/{barcode}.irma.decision', barcode=config['barcodes'].keys())
    output:
        "hadoop/all_allAlleles_realign.txt"
    log:
        "log/cat_irma/allAlleles.log"
    benchmark:
        "log/benchmarks/catallalleles_all.log"
    shell:
        'hostname && python {workflow.basedir}/scripts/addColumns.py -f IRMA/*/tables/SARS-CoV-2-allAlleles.txt -o {output} -n "\s+" --sc2 2> {log} || touch {output}'

rule cat_coverage:
    input:
        expand('IRMA/{barcode}.irma.decision', barcode=config['barcodes'].keys())
    output:
        'hadoop/all_coverage_realign.txt'
    log:
        "log/cat_irma/coverage.log"
    benchmark:
        "log/benchmarks/catcoverage_all.log"
    shell:
        'hostname && python {workflow.basedir}/scripts/addColumns.py -f IRMA/*/tables/SARS-CoV-2-coverage.pad.txt -o {output} --sc2 2> {log} || touch {output}'


rule cat_insertions:
    input:
        expand('IRMA/{barcode}.irma.decision', barcode=config['barcodes'].keys())
    output:
        'hadoop/all_insertions_realign.txt'
    log:
        "log/cat_irma/insertions.log"
    benchmark:
        "log/benchmarks/catinsertions_all.log"
    shell:
        'hostname && python {workflow.basedir}/scripts/addColumns.py -f IRMA/*/tables/SARS-CoV-2-insertions.txt -o {output} -n "\s+" --sc2 2> {log} || touch {output}'

rule cat_deletions:
    input:
        expand('IRMA/{barcode}.irma.decision', barcode=config['barcodes'].keys())
    output:
        'hadoop/all_deletions_realign.txt'
    log:
        "log/cat_irma/deletions.log"
    benchmark:
        "log/benchmarks/catdeletions_all.log"
    shell:
        'python {workflow.basedir}/scripts/addColumns.py -f IRMA/*/tables/SARS-CoV-2-deletions.txt -o {output} -n "\s+" --sc2 2> {log} || touch {output}'

rule irmaConsensus2hadoop:
    input:
        "hadoop/all_allAlleles_realign.txt",
        "hadoop/all_coverage_realign.txt",
        "hadoop/all_deletions_realign.txt",
        "hadoop/all_insertions_realign.txt"
    output:
        touch('hadoop/irmaConsensus2hadoop.fin')
    log:
        "log/irmaConsensus2hadoop/all.log"
    benchmark:
        "log/benchmarks/irmaconsensus2hadoop_all.log"
    shell:
        'export PERL5LIB= && python {workflow.basedir}/scripts/irmaConsensus2hadoop.py {config[irma_module]} {config[machine]} {config[runid]}  2> {log}'

rule cdc_config:
    input:
        rules.irmaConsensus2hadoop.output
    output:
        touch('hadoop/parquet/config.fin')
    log:
        'logs/cdc_config/all.log'
    benchmark:
        'logs/benchmarks/cdcconfig_all.log'
    shell:
        'python {workflow.basedir}/scripts/config4hadoop.py'

#root='/'.join(workflow.basedir.split('/')[:-3])
root='/scicomp/home-pure/sars2seq/prod'

rule dais:
    input:
        rules.cdc_config.output
    output:
        touch('hadoop/dais.fin')
    benchmark:
        'log/benchmarks/dais_all.log'
    shell:
        '{root}/dais-ribosome/ribosome --module BETACORONAVIRUS hadoop/{config[runid]} hadoop/{config[runid]}.seq hadoop/{config[runid]}.ins hadoop/{config[runid]}.del hadoop/{config[runid]}.gen' #add call later

rule finishup:
    input:
        passed_irma
    output:
        temp("SC2.{barcode}.fin")
    shell:
        "touch {output}"
