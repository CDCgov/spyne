#!/usr/bin/env snakemake


import os
import sys

import snakemake


shell.executable("bash")


workdir: "./"
configfile: "tests/config.yaml"
# report: "report/workflow.rst" https://snakemake.readthedocs.io/en/stable/snakefiles/reporting.html


localrules:
    gather_fastqs,
    summarize_barcode_trim_left,
    summarize_barcode_trim_right,
    summarize_cutadapt,
    summarize_subsample,
    report_assembly_statistics


rule all:
    input:
        expand("{barcode}.fin", barcode=config["barcodes"].keys()),
        # expand("IRMA/{barcode}.fastq", barcode=config["barcodes"].keys()),
        # expand("logs/{barcode}.bbduk.trim_left.stderr.log", barcode=config["barcodes"].keys()),
        # expand("logs/{barcode}.bbduk.trim_right.stderr.log", barcode=config["barcodes"].keys()),
        # expand("logs/{barcode}.cutadapt.stdout.log", barcode=config["barcodes"].keys()),
        # expand("logs/{barcode}.reformat.stderr.log", barcode=config["barcodes"].keys()),
        # expand("qa/stats.{barcode}.subsample.tsv", barcode=config["barcodes"].keys()),
        # expand("qa/{barcode}-assembly-stats.tsv", barcode=config["barcodes"].keys()),
        expand("qa/{barcode}.png", barcode=config["barcodes"].keys()),
        # expand("qa/stats.{barcode}_left_barcode_trimming.tsv", barcode=config["barcodes"].keys()),
        # expand("qa/stats.{barcode}_right_barcode_trimming.tsv", barcode=config["barcodes"].keys()),
        # expand("qa/stats.{barcode}.cutadapt.tsv", barcode=config["barcodes"].keys()),
        # expand("qa/stats.{barcode}.subsample.tsv", barcode=config["barcodes"].keys()),
        # expand("qa/{barcode}-assembly-stats.tsv", barcode=config["barcodes"].keys()),
        # ancient("IRMA/IRMA_reads2hadoop.fin"),
        # ancient("hadoop/amplicon2hadoop.fin")


ruleorder: gather_fastqs > barcode_trim_left > barcode_trim_right > irma > amplicov


rule gather_fastqs:
    """
    gathers all plaintext and compressed FastQ files into a single .fastq.gz file
    """
    # TO-DO: handle both automatically - plain and gz
    input:
        fastqs = lambda wildcards: config["barcodes"][wildcards.barcode]["fastqs"]
    output:
        "IRMA/{barcode}.fastq"
        # expand("IRMA/{barcode}.fastq", barcode=config["barcodes"].keys())
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step 1 - merging all read files into a single file"
    shell:
        "zcat tests/data/tiny-test.fastq.gz > {output}"


rule plot_read_lengths:
    """
    creates a read length distribution figure and embeds statistics such as
    N50, total count and length, median, mean, stdev, etc.
    """
    input:
        # "IRMA/{barcode}.fastq"
        expand("IRMA/{barcode}.fastq", barcode=config["barcodes"].keys())
    output:
        "qa/{barcode}.png"
    log:
        out = "logs/{barcode}.plot-read-lengths.stdout.log",
        err = "logs/{barcode}.plot-read-lengths.stderr.log"
    conda:
        "envs/plot.read-lengths.yaml"
    threads:
        snakemake.utils.available_cpu_count()
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step X - plotting sequence read lengths"
    shell:
        "scripts/plot.read-lengths.py"
        " --indir IRMA"
        " --prefix {barcode}"
        " --cpus {threads}"
        " --outfile {output}"
        " --title 'Sample_ID and Experiment_ID'"
        " --subtitle {barcode}"
        " 1> {log.out}"
        " 2> {log.err}"


rule barcode_trim_left:
    """
    5' barcode trim of reads allowing for 3 SNPs with 17mer searches
    """
    input:
        "IRMA/{barcode}.fastq"
    output:
        "IRMA/{barcode}_bartrim_l.fastq"
    log:
        out = "logs/{barcode}.bbduk.trim_left.stdout.log",
        err = "logs/{barcode}.bbduk.trim_left.stderr.log"
    params:
        barcode_sequence = lambda wildcards: config["barcodes"][wildcards.barcode]["barcode_sequence"]
    group:
        "trim-map"
    # conda:
    #     "envs/bbtools.yaml"
    threads:
        16
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step 2 - trimming left barcode"
    shell:
        "/apps/x86_64/bbmap/38.84/bbduk.sh"
        " in={input}"
        " out={output}"
        " hdist=3"
        " literal={params.barcode_sequence}"
        " ktrim=l"
        " k=17"
        " qin=33"
        " rcomp=f"
        " threads={threads}"
        " 1> {log.out}"
        " 2> {log.err}"


rule summarize_barcode_trim_left:
    """
    summarize 5' barcode trimming input and output read counts and basepair totals
    """
    input:
        "logs/{barcode}.bbduk.trim_left.stderr.log"
        # expand("logs/{barcode}.bbduk.trim_left.stderr.log", barcode=config["barcodes"].keys())
    output:
        "qa/stats.{barcode}_left_barcode_trimming.tsv"
    group:
        "trim-map"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step X - summarizing left barcode trimming"
    shell:
        "scripts/grep_summarize_bbduk_log.sh {input} > {output}"


rule barcode_trim_right:
    """
    3' barcode trim of reads allowing for 3 SNPs with 17mer searches
    """
    input:
        "IRMA/{barcode}_bartrim_l.fastq"
    output:
        "IRMA/{barcode}_bartrim_lr.fastq"
    log:
        out = "logs/{barcode}.bbduk.trim_right.stdout.log",
        err = "logs/{barcode}.bbduk.trim_right.stderr.log"
    params:
        barcode_sequence = lambda wildcards: config["barcodes"][wildcards.barcode]["barcode_sequence"]
    group:
        "trim-map"
    # conda:
    #     "envs/bbtools.yaml"
    threads:
        16
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step 3 - trimming right barcode"
    shell:
        "/apps/x86_64/bbmap/38.84/bbduk.sh"
        " in={input}"
        " out={output}"
        " hdist=3"
        " literal={params.barcode_sequence}"
        " ktrim=r"
        " k=17"
        " qin=33"
        " rcomp=f"
        " threads={threads}"
        " 1> {log.out}"
        " 2> {log.err}"


rule summarize_barcode_trim_right:
    """
    summarize 3' barcode trimming input and output read counts and basepair totals
    """
    input:
        "logs/{barcode}.bbduk.trim_right.stderr.log"
        # expand("logs/{barcode}.bbduk.trim_right.stderr.log", barcode=config["barcodes"].keys())
    output:
        "qa/stats.{barcode}_right_barcode_trimming.tsv"
    group:
        "trim-map"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step X - summarizing right barcode trimming"
    shell:
        "scripts/grep_summarize_bbduk_log.sh {input} > {output}"


rule cutadapt:
    """
    remove 30 bp from 5' and 3' end of each read
    """
    input:
        "IRMA/{barcode}_bartrim_lr.fastq"
    output:
        "IRMA/{barcode}_bartrim_lr_cutadapt.fastq"
    log:
        out = "logs/{barcode}.cutadapt.stdout.log",
        err = "logs/{barcode}.cutadapt.stderr.log"
    group:
        "trim-map"
    conda:
        "envs/cutadapt.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step 4 - clipping reads on both sides"
    shell:
        "cutadapt -u 30 -u -30 -o {output} {input} 1> {log.out} 2> {log.err}"


rule summarize_cutadapt:
    """
    summarize input and output counts from removing 30 bp from 5' and 3' read ends
    """
    input:
        "logs/{barcode}.cutadapt.stdout.log"
        # expand("logs/{barcode}.cutadapt.stdout.log", barcode=config["barcodes"].keys())
    output:
        "qa/stats.{barcode}.cutadapt.tsv"
    group:
        "trim-map"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step X - summarizing cutadapt trimming"
    shell:
        """
        cnt_input_reads=$(grep '^Total reads' {input} | awk '{print $4}')
        cnt_input_bases=$(grep '^Total basepairs' {input} | awk '{print $4}' | sed 's/,//g')
        cnt_adapters=$(grep '^Reads with adapters:' {input} | awk '{print $4}' | sed 's/,//g')
        cnt_output_reads=$(( "${cnt_input_reads}" - "${cnt_adapters}" ))
        cnt_output_bases=$(grep '^Total written' {input} | awk '{print $4}' | sed 's/,//g')
        nfo="${cnt_input_reads} input reads\t${cnt_input_bases} input bases\t"
        nfo+="${cnt_output_reads} output reads \t${cnt_output_bases} output bases"
        echo -e "${nfo}" > {output}
        """


# rule subsample:
#     input:
#         "IRMA/{barcode}_bartrim_lr_cutadapt.fastq"
#     output:
#         "IRMA/{barcode}_bartrim_lr_cutadapt.fastq"
#     log:
#         # out = "logs/{barcode}.seqtk.stdout.log",
#         err = "logs/{barcode}.seqtk.stderr.log"
#     group:
#         "trim-map"
#     conda:
#         "envs/seqtk.yaml"
#     message:
        "Step 4b - subsampling cleaned up read if excess > 1M exist"
#     shell:
#         "seqtk sample -s608 {input} 1000000 1> {output} 2> {log.err}"


rule subsample:
    """
    if there are > 1 million reads, randomly subsample 1 million sequences
    """
    input:
        "IRMA/{barcode}_bartrim_lr_cutadapt.fastq"
    output:
        "IRMA/{barcode}_bartrim_lr_cutadapt_subsampled.fastq"
    log:
        out = "logs/{barcode}.reformat.stdout.log",
        err = "logs/{barcode}.reformat.stderr.log"
    group:
        "trim-map"
    # conda:
    #     "envs/bbtools.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step 4b - subsampling cleaned up read if excess > 1M exist"
    shell:
        "/apps/x86_64/bbmap/38.84/reformat.sh"
        " in={input}"
        " out={output}"
        " samplereadstarget=1000000"
        " qin=33"
        " 1> {log.out}"
        " 2> {log.err}"


rule summarize_subsample:
    """
    summarize input and output counts from subsampling
    """
    input:
        "logs/{barcode}.reformat.stderr.log"
        # expand("logs/{barcode}.reformat.stderr.log", barcode=config["barcodes"].keys())
    output:
        "qa/stats.{barcode}.subsample.tsv"
    group:
        "trim-map"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step X - summarizing subsampling"
    shell:
        """
        cnt_input_reads=$(grep '^Input:' {input} | awk '{print $2}')
        cnt_input_bases=$(grep '^Input:' {input} | awk '{print $5}')
        cnt_output_reads=$(grep '^Output:' {input} | awk '{print $2}')
        cnt_output_bases=$(grep '^Output:' {input} | awk '{print $5}')
        cnt_discarded_reads=$(( "${cnt_input_reads}" - "${cnt_output_reads}" ))
        cnt_discarded_bases=$(( "${cnt_input_bases}" - "${cnt_output_bases}" ))
        nfo="${cnt_discarded_reads} discarded reads\t"
        nfo+="${cnt_discarded_bases} discarded bases"
        echo -e "${nfo}" > {output}
        """


rule irma:
    """
    assemble genome with an iterative refinement reference-based process
    """
    input:
        "IRMA/{barcode}_bartrim_lr_cutadapt_subsampled.fastq"
    output:
        touch("IRMA/{barcode}.irma.fin")
    log:
        out = "logs/{barcode}.irma.stdout.log",
        err = "logs/{barcode}.irma.stderr.log"
    group:
        "trim-map"
    threads:
        16
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step 5 - assembling genome"
    shell:
        "scripts/irmawrapper.sh"
        " {config[irmamodule]}"
        " {input}"
        " {wildcards.barcode}"
        " 1> {log.out}"
        " 2> {log.err}"


rule quast:
    """
    calculate a variety of quality assessment statistics on the genome
    assembly such as cumulative length, contig quantity, GC-content, N50, etc.
    """
    input:
        "IRMA/{barcode}/SARS-CoV-2.fasta"
        # expand("IRMA/{barcode}/SARS-CoV-2.fasta", barcode=config["barcodes"].keys())
    output:
        path = temp(directory("qa/.tmp/{barcode}")),
        file = temp("qa/.tmp/{barcode}/transposed_report.tsv")
    log:
        out = "logs/{barcode}.quast.stdout.log",
        err = "logs/{barcode}.quast.stderr.log"
    group:
        "trim-map"
    conda:
        "envs/quast.yaml"
    threads:
        16
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    message:
        "Step 5b - calculating assembly metrics"
    shell:
        "quast"
        " --fast"
        " --threads {threads}"
        " --output-dir {output.path}"
        " {input}"
        " 1> {log.out}"
        " 2> {log.err}"


rule report_assembly_statistics:
    """
    use pre-computed quast output and summarize to only the statistics of
    interest which is the entire transposed report for now
    """
    input:
        "qa/.tmp/{barcode}/transposed_report.tsv"
        # expand("qa/.tmp/{barcode}/transposed_report.tsv", barcode=config["barcodes"].keys())
    output:
        "qa/{barcode}-assembly-stats.tsv"
    group:
        "trim-map"
    message:
        "Step 5c - moving assembly metrics"
    shell:
        "mv {input} {output}"


checkpoint checkirma:
    """
    Pipeline waits here for all samples to produce the checkpoint input needed
    here, and it then reevaluates the needed DAG for each sample.
    """
    input:
        ancient('IRMA/{barcode}.irma.fin')
    output:
        'IRMA/{barcode}.irma.decision'
    shell:
        "[ -s IRMA/{wildcards.barcode}/SARS-CoV-2.bam ] &&"
        " echo passed > {output} ||"
        " echo failed > {output}"


rule irmareads2hadoop:
    """
    ?????????
    """
    input:
        ancient('IRMA/{barcode}.irma.decision')
        # ancient(expand('IRMA/{barcode}.irma.decision', barcode=config['barcodes'].keys()))
    output:
        temp('IRMA/IRMA_reads2hadoop.fin')
    log:
        out = "logs/irmareads2hadoop.stdout.log",
        err = "logs/irmareads2hadoop.stderr.log"
    conda:
        "envs/hadoop.yaml"
    benchmark:
        "logs/benchmarks/{name}.log"
    shell:
        "python scripts/irmaBasicReadStats2hadoop.py"
        " -s {input}"
        " 1> {log.out}"
        " 2> {log.err}"
        " && touch {output}"


def passed_irma(wildcards):
    with checkpoints.checkirma.get(barcode=wildcards.barcode).\
    output[0].open() as f:
        if f.read().strip() == "passed":
            return 'hadoop/dais.fin'
        else:
            return "IRMA_negative/{barcode}"


rule pass_negatives:
    """
    ?????????
    """
    input:
        "IRMA/{barcode}.irma.decision"
    output:
        "IRMA_negative/{barcode}"
    shell:
        "touch {output}"


rule sort_bam:
    """
    ?????????
    """
    input:
        "IRMA/{barcode}.irma.decision"
    output:
        "IRMA/{barcode}_SARS-CoV-2_sorted.bam"
    threads:
        14
    group:
        "amplicon-cov"
    log:
        out = "logs/{barcode}.sort-bam.stdout.log",
        err = "logs/{barcode}.sort-bam.stderr.log"
    conda:
        "envs/samtools.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    shell:
        "hostname &&"
        " samtools sort"
        " IRMA/{barcode}/SARS-CoV-2.bam"
        " -o {output}"
        " --threads {threads}"
        " 1> {log.out}"
        " 2> {log.err}"
        " || touch {output}"


# Set library primers and PCR Library files according to Library in config
primers = {
    "swift": {
        "bedpe": "/scicomp/home-pure/sars2seq/Resources/primers/SNAP_v2_amplicon_panel_IRMAref.bedpe",
        "fasta": "/scicomp/home-pure/sars2seq/Resources/primers/SNAP_v2_amplicon_panel.fasta",
    },
    "4pool": {
        "bedpe": "/scicomp/home-pure/sars2seq/Resources/primers/SC2_200710-bypool200814_IRMAref.bedpe",
        "fasta": "/scicomp/home-pure/sars2seq/Resources/primers/SC2_200710-bypool200814.fasta",
    },
    "articv3": {
        "bedpe": "/scicomp/home-pure/sars2seq/Resources/primers/artic_v3.bedpe",
        "fasta": "/scicomp/home-pure/sars2seq/Resources/primers/artic_v3.fasta",
    },
    "sgene_v1": {
        "bedpe": "/scicomp/home-pure/sars2seq/Resources/primers/sgene_v1.bedpe",
        "fasta": "/scicomp/home-pure/sars2seq/Resources/primers/sgene_v1.fasta",
    },
}


rule amplicov:
    """
    ?????????
    """
    input:
        "IRMA/{barcode}_SARS-CoV-2_sorted.bam"
        # expand("IRMA/{barcode}_SARS-CoV-2_sorted.bam", barcode=config["barcodes"].keys())
    output:
        "IRMA/{barcode}_amplicon_coverage.txt"
    group:
        "amplicon-cov"
    log:
        out = "logs/{barcode}.amplicov.stdout.log",
        err = "logs/{barcode}.amplicov.stderr.log"
    benchmark:
        "logs/benchmarks/amplicov_{barcode}.log"
    conda:
        "envs/hadoop.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    params:
        bedpe = lambda wildcards: primers[config["barcodes"][wildcards.barcode]["Library"]]["bedpe"]
    shell:
        "hostname &&"
        " [ -s {input} ] &&"
        " python scripts/amplicov.py"
        " --bedpe {params.bedpe}"
        " --bam {input}"
        " -o IRMA"
        " -p {barcode}"
        " 1> {log.out}"
        " 2> {log.err}"
        " && touch {output}"
        " || touch {output}"


rule cat_amplicov:
    """
    ?????????
    """
    input:
        "IRMA/{barcode}_amplicon_coverage.txt"
        # expand("IRMA/{barcode}_amplicon_coverage.txt", barcode=config["barcodes"].keys())
    output:
        "hadoop/all_amplicon_coverage.txt"
    group:
        "amplicov2hadoop"
    log:
        out = "logs/{barcode}.catamplicov.stdout.log",
        err = "logs/{barcode}.catamplicov.stderr.log"
    benchmark:
        "logs/benchmarks/catamplicov_all.log"
    conda:
        "envs/hadoop.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    shell:
        "hostname &&"
        " python scripts/addColumns.py"
        " -f {input}"
        " -o {output}"
        " --sc2"
        " 1> {log.out}"
        " 2> {log.err}"
        " && touch {output}"
        " || touch {output}"


rule amplicov2hadoop:
    """
    ?????????
    """
    input:
        "hadoop/all_amplicon_coverage.txt"
    output:
        "hadoop/amplicon2hadoop.fin"
    group:
        "amplicon2hadoop"
    log:
        out = "logs/hadoopload.stdout.log",
        err = "logs/hadoopload.stderr.log"
    benchmark:
        "logs/benchmarks/{name}.log"
    shell:
        "hostname"
        " && touch {output}" #add hput and himpala later


rule cat_allAlleles:
    """
    ?????????
    """
    input:
        "IRMA/{barcode}.irma.decision"
        # expand("IRMA/{barcode}.irma.decision", barcode=config["barcodes"].keys())
    output:
        "hadoop/all_allAlleles_realign.txt"
    log:
        out = "logs/catallalleles.stdout.log",
        err = "logs/catallalleles.stderr.log"
    benchmark:
        "logs/benchmarks/catallalleles_all.log"
    conda:
        "envs/hadoop.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    shell:
        "hostname &&"
        " python scripts/addColumns.py"
        " -f IRMA/*/tables/SARS-CoV-2-allAlleles.txt"
        " -o {output}"
        " --sc2"
        " 1> {log.out}"
        " 2> {log.err}"
        " || touch {output}"


rule cat_coverage:
    """
    ?????????
    """
    input:
        "IRMA/{barcode}.irma.decision"
        # expand("IRMA/{barcode}.irma.decision", barcode=config["barcodes"].keys())
    output:
        "hadoop/all_coverage_realign.txt"
    log:
        out = "logs/catcoverage.stdout.log",
        err = "logs/catcoverage.stderr.log"
    benchmark:
        "logs/benchmarks/catcoverage_all.log"
    conda:
        "envs/hadoop.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    shell:
        "hostname &&"
        " python scripts/addColumns.py"
        " -f IRMA/*/tables/SARS-CoV-2-coverage.pad.txt"
        " -o {output}"
        " --sc2"
        " 2> {log.out}"
        " 2> {log.err}"
        " || touch {output}"


rule cat_insertions:
    """
    ?????????
    """
    input:
        "IRMA/{barcode}.irma.decision"
        # expand("IRMA/{barcode}.irma.decision", barcode=config["barcodes"].keys())
    output:
        "hadoop/all_insertions_realign.txt"
    log:
        out = "logs/insertions.stdout.log",
        err = "logs/insertions.stderr.log"
    benchmark:
        "logs/benchmarks/catinsertions_all.log"
    conda:
        "envs/hadoop.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    shell:
        "hostname &&"
        " python scripts/addColumns.py"
        " -f IRMA/*/tables/SARS-CoV-2-insertions.txt"
        " -o {output}"
        " --sc2"
        " 1> {log.out}"
        " 2> {log.err}"
        " || touch {output}"


rule cat_deletions:
    """
    ?????????
    """
    input:
        "IRMA/{barcode}.irma.decision"
        # expand("IRMA/{barcode}.irma.decision", barcode=config["barcodes"].keys())
    output:
        "hadoop/all_deletions_realign.txt"
    log:
        out = "logs/deletions.stdout.log",
        err = "logs/deletions.stderr.log"
    benchmark:
        "logs/benchmarks/catdeletions_all.log"
    conda:
        "envs/hadoop.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    shell:
        "python scripts/addColumns.py"
        " -f IRMA/*/tables/SARS-CoV-2-deletions.txt"
        " -o {output}"
        " --sc2"
        " 1> {log.out}"
        " 2> {log.err}"
        " || touch {output}"


rule irmaConsensus2hadoop:
    """
    ?????????
    """
    input:
        "hadoop/all_allAlleles_realign.txt",
        "hadoop/all_coverage_realign.txt",
        "hadoop/all_deletions_realign.txt",
        "hadoop/all_insertions_realign.txt"
    output:
        touch("hadoop/irmaConsensus2hadoop.fin")
    log:
        out = "logs/assembly-consensus2hadoop.stdout.log",
        err = "logs/assembly-consensus2hadoop.stderr.log"
    benchmark:
        "logs/benchmarks/irmaconsensus2hadoop_all.log"
    conda:
        "envs/hadoop.yaml"
    benchmark:
        "logs/benchmarks/{barcode}.{name}.tsv"
    shell:
        "export PERL5LIB='' &&"
        " python scripts/irmaConsensus2hadoop.py"
        " {config[irmamodule]}"
        " {config[machine]}"
        " {config[runid]}"
        " 1> {log.out}"
        " 2> {log.err}"


root = "/".join(workflow.basedir.split("/")[:-3])


rule dais:
    """
    ?????????
    """
    input:
        rules.irmaConsensus2hadoop.output
    output:
        touch('hadoop/dais.fin')
    benchmark:
        'logs/benchmarks/{name}.log'
    shell:
        'echo dais' #add call later


rule irmaroundup:
    """
    ?????????
    """
    input:
        passed_irma
    output:
        temp("{barcode}.fin")
    shell:
        "touch {output}"

onsuccess:
    shell("date '+%Y-%b-%d %a %H:%M:%S'")
